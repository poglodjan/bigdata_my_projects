{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Spark Python Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark --master local[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Reading csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = spark.read.format('csv').options(header='true',\n",
    "                                      inferschema='true').load(\n",
    "                                          'hdfs://localhost:8020/user/vagrant/test2/spark/matches.csv'\n",
    "                                      )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Filtering, grouping, joining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.filter(df['Date'] == '2015-03-01 00:00:00')\n",
    "df2 = df.filter(df['Date'] == '2015-03-02 00:00:00')\n",
    "print(\"df1 count = \" + str(df1.count()))\n",
    "print(\"df2 count = \" + str(df2.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.groupBy('TimePeriod').agg({'Flow':'sum','AverageSpeed':'max'})\n",
    "df2 = df2.groupBy('TimePeriod').agg({'Flow':'sum','AverageSpeed':'max'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2, \n",
    "         df1['TimePeriod'] == df2['TimePeriod']).filter(\n",
    "             df1['max(AverageSpeed)']>df2['max(AverageSpeed)']*1.25\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Spark Python script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Load text file ond HDFS and do word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # creating spark context\n",
    "    conf = SparkConf().setAppName(\"Word Count\")\n",
    "    sc = SparkContext(conf = conf)\n",
    "    \n",
    "    # reading files and spliting each document into words\n",
    "    words = sc.textFile(\"file:///user/vagrant/data/data.csv\").flatMap(\n",
    "        lambda line: line.split(\" \"))\n",
    "    \n",
    "    # count the occurence of each word\n",
    "    wordCounts = words.map(lambda word: (word, 1)).reduceByKey(\n",
    "        lambda a,b: a+b).sortBy(lambda a: a[1])\n",
    "    wordCounts.saveAsTextFile(\"file:///user/vagrant/data/output.cnt\")\n",
    "    print(\"completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## submit a spark job and validate results stored in HDFS:\n",
    "spark-submit --master local[2] wordcount.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Configuring a logger to record program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Logging confuguration\n",
    "formatter = logging.Formatter('[%(asctime)s] %(levelname)s @ line %(lineno)d: %(message)s')\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "handler.setFormatter(formatter)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.IFO)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "def main():\n",
    "    #start spark session\n",
    "    spark = SparkSession.builder.appName(\"SparkDemo\").getOrdCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    logger.info(\"Starting spark application\")\n",
    "    logger.info(\"Reading CSV File\")\n",
    "    \n",
    "    df = spark.read.option(\"header\",\"true\").option(\n",
    "        \"inferschema\",\"true\"\n",
    "    ).csv(\"hdfs://localhost:8020/user/vagrant/data/data.csv\")\n",
    "    logger.info(\"previewing csv file\")\n",
    "    df.show()\n",
    "    \n",
    "    logger.info(\"Data Aggregation\")\n",
    "    df.filter(df[\"TimePeriod\"].between(10,12)).groupBy(\n",
    "        \"LinkRef\").agg({\"Flow\",\"avg\"}).show()\n",
    "    df.createOrReplaceTempView(\"RoadData\")\n",
    "\n",
    "    spark.sql(\"SELECT LinkRef, AVG(Flow) FROM RoadData WHERE \\\n",
    "              TimePeriod BETWEEN 10 AND 12 GROUPBY LinkRef\").show()\n",
    "    logger.info(\"Ending spark application\")\n",
    "    spark.stop()\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) findspark jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Jupyter Demo\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "df = spark.read.option(\"header\",\"true\").option(\n",
    "    \"inferschema\",\"true\").csv(\"hdfs://localhost:8020/user/vagrant/data.csv\")\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
