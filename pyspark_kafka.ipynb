{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pextra packages passed in the command line:\n",
    "pyspark --master local[2] --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Putting csv on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "data = {\n",
    "    \"id\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"firstname\": [\"Babita\", \"Zsa Zsa\", \"Vonny\", \"Ermengarde\", \"Karina\", \"Felice\", \"Elsie\", \"Kaia\", \"Glynnis\", \"Jany\"],\n",
    "    \"age\": [27, 39, 48, 24, 51, 36, 55, 36, 43, 28],\n",
    "    \"profession\": [\"Lawyer\", \"Musician\", \"Police Officer\", \"Teacher\", \"Software Developer\", \"Doctor\", \"Police Officer\", \"Police Officer\", \"Designer\", \"Lawyer\"],\n",
    "    \"city\": [\"Riverside\", \"Malé\", \"Bahía Blanca\", \"Porto Alegre\", \"Amritsar\", \"Montreal\", \"City of San Marino\", \"Gaza\", \"Hamburg\", \"Belize City\"],\n",
    "    \"salary\": [1558, 5667, 7612, 2451, 3522, 9874, 2231, 2263, 6983, 8769]\n",
    "}\n",
    "# Writing file locally\n",
    "csv_file_path = \"example_data.csv\"\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CSV to HDFS\").getOrCreate()\n",
    "hdfs_output_path = \"hdfs:///user/vagrant/kafka/data\"\n",
    "\n",
    "# Upload csv file to pyspark dataframe\n",
    "spark_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "spark_df.show()\n",
    "# saving data to csv in parquet format\n",
    "spark_df.write.mode(\"overwrite\").parquet(hdfs_output_path)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) streaming from a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schemal = StructType([StructField('id', IntegerType(), True),\n",
    "                    StructField('name', StringType(), True),\n",
    "                    StructField('age', IntegerType(), True),\n",
    "\t                StructField('profession', StringType(), True),\n",
    "\t                StructField('city', StringType(), True),\n",
    "\t                StructField('salary', DoubleType(), True)])\n",
    "\n",
    "customer = spark.readStream.format(\"csv\").schema(\"schemal\"). \\\n",
    "    option(\"header\",True).option(\"maxFilesPerTrigger\", 1). \\\n",
    "        load(\"/user/vagrant/kafka/data/\")\n",
    "\n",
    "average_salaries = customer. \\\n",
    "    groupBy(\"profession\"). \\\n",
    "    agg((avg(\"salary\").alias(\"average_salary\")), (count(\"profession\").alias(\"count\"))). \\\n",
    "    sort(desc(\"average_salary\"))\n",
    "\n",
    "query = average_salaries.writeStream.format(\"console\").outputMode(\"complete\").start()\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) streaming from kafka (receive live messages from a producer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df = spark.readStream.format(\"kafka\"). \\\n",
    "    option(\"kafka.bootstrap.servers\", \"localhost:9092\"). \\\n",
    "    option(\"subscribe\", \"streamtest\").load()\n",
    "df1 = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "schemap = StructType ([StructField('id', IntegerType(), True),\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "        StructField('lastname', StringType(), True),\n",
    "        StructField('dob_year', IntegerType(), True),\n",
    "        StructField('dob_month', IntegerType(), True),\n",
    "        StructField('gender', StringType(), True),\n",
    "        StructField('salary', IntegerType(), True)])\n",
    "\n",
    "# parsing\n",
    "pdf = df1.select(from_json(col(\"value\"), schemap).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "\"\"\"\n",
    "    q1 - to display on console\n",
    "    q2 - to save as csv in hdfs\n",
    "    q3 - to publish as another topic after some modifications\n",
    "\"\"\"\n",
    "\n",
    "q1 = pdf.writeStream.format(\"console\").outputMode(\"append\").start()\n",
    "# q1 awaitTermination()\n",
    "\n",
    "q2 = ( pdf.writeStream.format(\"csv\").outputMode(\"append\"). \n",
    "    option(\"path\", \"kafka/data/\"). \n",
    "    option(\"checkpointLocation\", \"/tmp/vagrant/checkpoint\").start() )\n",
    "\n",
    "mpdf1 = (\n",
    "        mpdf.selectExpr(\n",
    "            \"CAST(id AS STRING)\",\n",
    "            \"CAST(firstname AS STRING)\",\n",
    "            \"CAST(middlename AS STRING)\",\n",
    "            \"CAST(lastname AS STRING)\",\n",
    "            \"CAST(dob_year AS STRING)\",\n",
    "            \"CAST(dob_month AS STRING)\",\n",
    "            \"CAST(gender AS STRING)\",\n",
    "            \"CAST(salary AS STRING)\",\n",
    "        )\n",
    "        .withColumn(\"value\", to_json(struct(\"*\")).cast(\"string\"),)\n",
    "    )\n",
    "q3 = (\n",
    "        mpdf1\n",
    "        .select(\"value\")\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"kafka\")\n",
    "        .option(\"topic\", \"streamout\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "        .option(\"checkpointLocation\", \"/tmp/vagrant/checkpoint\")\n",
    "        .start()\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming data as producer:\n",
    "{\"id\":1,\"firstname\":\"James \",\"middlename\":\"\",\"lastname\":\"Smith\",\"dob_year\":2018,\"dob_month\":1,\"gender\":\"M\",\"salary\":3000}\n",
    "{\"id\":2,\"firstname\":\"Michael \",\"middlename\":\"Rose\",\"lastname\":\"\",\"dob_year\":2010,\"dob_month\":3,\"gender\":\"M\",\"salary\":4000}\n",
    "{\"id\":3,\"firstname\":\"Robert \",\"middlename\":\"\",\"lastname\":\"Williams\",\"dob_year\":2010,\"dob_month\":3,\"gender\":\"M\",\"salary\":4000}\n",
    "{\"id\":4,\"firstname\":\"Maria \",\"middlename\":\"Anne\",\"lastname\":\"Jones\",\"dob_year\":2005,\"dob_month\":5,\"gender\":\"F\",\"salary\":4000}\n",
    "{\"id\":5,\"firstname\":\"Jen\",\"middlename\":\"Mary\",\"lastname\":\"Brown\",\"dob_year\":2010,\"dob_month\":7,\"gender\":\"\",\"salary\":-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1.stop()\n",
    "q2.stop()\n",
    "q3.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Streaming from a socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.readStream.format(\"socket\"). \\\n",
    "    option(\"host\", \"localhost\"). \\\n",
    "    option(\"port\", 9999).load()\n",
    "lines.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the lines into words:\n",
    "words = lines.select(\n",
    "    explode(\n",
    "        split(lines.value, \" \")\n",
    "    ).alias(\"word\")\n",
    ")\n",
    "wordCounts = words.groupBy(\"word\").count().sort('count', ascending=False)\n",
    "\n",
    "query = wordCounts.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "#query.awaitTermination()\n",
    "#query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"kafka\"). \\\n",
    "    option(\"kafka.bootstrap.servers\", \"localhost:9092\"). \\\n",
    "    option(\"subscribe\", \"streamtest\").load()\n",
    "df1 = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "words = df1.select(\n",
    "    explode(\n",
    "        split(df1.value, \" \")\n",
    "    ).alias(\"word\")\n",
    ")\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    " \n",
    "# Filter words containing the letter 'z' or an email address\n",
    "filtered_words = words.filter(\n",
    "    (col(\"word\").rlike(\".*z.*\")) | (col(\"word\").rlike(\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"))\n",
    ")\n",
    " \n",
    "# Display filtered words\n",
    "query = filtered_words \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    " \n",
    "#query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Publish the counts as a kafka stream\n",
    "##### as key:value pairs - key is a word, value is the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2 = df1.select(\n",
    "    explode(\n",
    "        split(df1.value, \" \")\n",
    "    ).alias(\"key\")\n",
    ")\n",
    "\n",
    "wodCounts2 = words2.grouBy(\"key\").count(). \\\n",
    "    withColumnRenamed(\"count\", \"value\"). \\\n",
    "    withColumn(\"value\",col(\"value\").cast(StringType()))\n",
    "\n",
    "query2 = ( wordCounts2.writeStream.\n",
    "          format(\"kafka\").outputMode(\"update\").\n",
    "            option(\"kafka.bootstrap.servers\", \"localhost:9092\").\n",
    "            option(\"topic\", \"streamout\").\n",
    "            option(\"checkpointLocation\", \"/tmp/vagrant/checkpoint\")            \n",
    ")\n",
    "\n",
    "#query2.awaitTermination()\n",
    "#query2.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
